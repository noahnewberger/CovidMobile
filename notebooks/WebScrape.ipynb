{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from bs4 import Tag, NavigableString\n",
    "import re\n",
    "import requests as r\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "from state_cleaner import *\n",
    "from selenium import webdriver\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/samismalling/Documents/mobility-report-data-extractor-master/CovidMobile'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('../..')\n",
    "os.chdir('CovidMobile')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_save(file_path):\n",
    "    path = ('data/{}'.format(file_path))\n",
    "    if os.path.exists(path):\n",
    "        os.remove(path)\n",
    "    else:\n",
    "        print(\"{} does not exist\".format(file_path))\n",
    "    \n",
    "\n",
    "def write_out(file_path, url):\n",
    "    path = ('data/{}'.format(file_path))\n",
    "    print(file_path)\n",
    "    with open(path, mode='w', newline='') as file:\n",
    "        file_writer = csv.writer(file, delimiter=',')\n",
    "        print(url)\n",
    "        with r.get(url, verify=False, stream=True) as res:\n",
    "            lines = (line.decode('utf-8',errors='ignore').replace('\\0','') for line in res.iter_lines())\n",
    "            for row in csv.reader(lines):\n",
    "                print(row)\n",
    "                file_writer.writerow(row)\n",
    "\n",
    "                \n",
    "                \n",
    "def create_stuff(file_path,url):\n",
    "    check_save(file_path)\n",
    "    write_out(file_path,url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Mobility Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.gstatic.com/covid19/mobility/Global_Mobility_Report.csv\n"
     ]
    }
   ],
   "source": [
    "#Downloads the .csv file of all mobility data from the google mobility site\n",
    "goog_files = os.getcwd()+'/data'\n",
    "my_url = 'https://www.google.com/covid19/mobility/'\n",
    "\n",
    "html= r.get(my_url)\n",
    "soup = bs(html.content,'lxml')\n",
    "current_link = ''\n",
    "for link in soup.find_all('a'):\n",
    "    current_link = link.get('href')\n",
    "    if current_link.endswith('csv'):\n",
    "        print(current_link)\n",
    "        myfile = r.get(current_link)\n",
    "        open('{}/{}'.format(goog_files, current_link.rsplit('/', 1)[1]),'wb').write(myfile.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARDA CSV Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\Noah\\\\CovidMobile/ARDA_Congregation_Data.csv' -> 'C:\\\\Users\\\\Noah\\\\CovidMobile/data/ARDA_Congregation_Data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-99101601db84>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m#move file into data folder:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/ARDA_Congregation_Data.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/data/ARDA_Congregation_Data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\Noah\\\\CovidMobile/ARDA_Congregation_Data.csv' -> 'C:\\\\Users\\\\Noah\\\\CovidMobile/data/ARDA_Congregation_Data.csv'"
     ]
    }
   ],
   "source": [
    "#arda_files = os.getcwd()+'/data'\n",
    "my_url = 'http://www.thearda.com/Archive/Files/Downloads/RCMSMGCY_DL2.asp'\n",
    "\n",
    "html= r.get(my_url)\n",
    "soup = bs(html.content,'lxml')\n",
    "current_link = ''\n",
    "for link in soup.find_all('a'):\n",
    "    current_link = link.get('href')\n",
    "    if '2658v' in current_link:\n",
    "        xlsx_file = r.get(current_link)\n",
    "        output = open('ARDA_Congregation_Data.csv', 'wb')\n",
    "        output.write(xlsx_file.content)\n",
    "        output.close()\n",
    "        \n",
    "#move file into data folder:\n",
    "path = os.getcwd()\n",
    "os.rename(path+'/ARDA_Congregation_Data.csv', path+'/data/ARDA_Congregation_Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closures and SAH Order Dates:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from: https://www.kff.org/report-section/state-data-and-policy-actions-to-address-coronavirus-sources/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.kff.org/report-section/state-data-and-policy-actions-to-address-coronavirus-sources/'\n",
    "html= r.get(url)\n",
    "soup = bs(html.content,'lxml')\n",
    "\n",
    "states = [str(x.text).strip('<b>/') for x in soup.findAll('b')]\n",
    "#closure_dates = pd.DataFrame(states, columns = ['States'])\n",
    "\n",
    "text = [str(x.text).strip('\\n\\t<b>/') for x in soup.findAll(['b', 'span'])]\n",
    "text = text[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do the rest in a loop\n",
    "for i, state in enumerate(states):\n",
    "    this_state = text.index(state)\n",
    "    next_state = text.index(states[i])\n",
    "    for line in text[this_state+1:next_state]:\n",
    "        if line[0].isdigit() and re.search('[A-Z]', line):\n",
    "            d = line[0:re.search('[A-Z]', line).start()-1]\n",
    "            w = line[re.search('[A-Z]', line).start():]\n",
    "            new_line = pd.Series({'Date': d, 'Description': w, 'State': state})\n",
    "            closure_dates = closure_dates.append(new_line,ignore_index=True)\n",
    "            \n",
    "this_state = text.index('WYOMING')\n",
    "for line in text[this_state+1:len(text)-1]:\n",
    "    if line[0].isdigit() and re.search('[A-Z]', line):\n",
    "        d = line[0:re.search('[A-Z]', line).start()-1]\n",
    "        w = line[re.search('[A-Z]', line).start():]\n",
    "        new_line = pd.Series({'Date': d, 'Description': w, 'State': 'WYOMING'})\n",
    "        closure_dates = closure_dates.append(new_line,ignore_index=True)\n",
    "        \n",
    "closure_dates.to_csv('/Users/samismalling/Documents/mobility-report-data-extractor-master/CovidMobile/data/closure_dates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Date</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>3/13</td>\n",
       "      <td>Public Health Emergency:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>3/17, 3/19, 3/20, 3/26, 3/27</td>\n",
       "      <td>School Closures, Large Gatherings Ban, Bar/Res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>3/18</td>\n",
       "      <td>Primary Election Postponement:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>3/23</td>\n",
       "      <td>Section 1135 Waiver:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>3/27, 4/3</td>\n",
       "      <td>Non-Essential Business Closures, Stay At Home ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2714</th>\n",
       "      <td>WYOMING</td>\n",
       "      <td>3/13</td>\n",
       "      <td>Emergency Declaration, Public Health Emergency:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2715</th>\n",
       "      <td>WYOMING</td>\n",
       "      <td>3/19, 3/27, 4/3</td>\n",
       "      <td>School Closures, Bar/Restaurant Limits:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2716</th>\n",
       "      <td>WYOMING</td>\n",
       "      <td>3/20, 3/27, 4/3</td>\n",
       "      <td>Large Gatherings Ban:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2717</th>\n",
       "      <td>WYOMING</td>\n",
       "      <td>3/27</td>\n",
       "      <td>Section 1135 Waiver:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2718</th>\n",
       "      <td>WYOMING</td>\n",
       "      <td>4/3</td>\n",
       "      <td>Mandatory Quarantine for Travelers:</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2719 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        State                          Date  \\\n",
       "0     ALABAMA                          3/13   \n",
       "1     ALABAMA  3/17, 3/19, 3/20, 3/26, 3/27   \n",
       "2     ALABAMA                          3/18   \n",
       "3     ALABAMA                          3/23   \n",
       "4     ALABAMA                     3/27, 4/3   \n",
       "...       ...                           ...   \n",
       "2714  WYOMING                          3/13   \n",
       "2715  WYOMING               3/19, 3/27, 4/3   \n",
       "2716  WYOMING               3/20, 3/27, 4/3   \n",
       "2717  WYOMING                          3/27   \n",
       "2718  WYOMING                           4/3   \n",
       "\n",
       "                                            Description  \n",
       "0                             Public Health Emergency:   \n",
       "1     School Closures, Large Gatherings Ban, Bar/Res...  \n",
       "2                       Primary Election Postponement:   \n",
       "3                                 Section 1135 Waiver:   \n",
       "4     Non-Essential Business Closures, Stay At Home ...  \n",
       "...                                                 ...  \n",
       "2714   Emergency Declaration, Public Health Emergency:   \n",
       "2715           School Closures, Bar/Restaurant Limits:   \n",
       "2716                             Large Gatherings Ban:   \n",
       "2717                              Section 1135 Waiver:   \n",
       "2718                Mandatory Quarantine for Travelers:  \n",
       "\n",
       "[2719 rows x 3 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closure_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Harvard Voting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "url= 'https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/VOQCHQ'\n",
    "driver = webdriver.Safari()\n",
    "driver.get(url)\n",
    "\n",
    "\n",
    "download = driver.find_element_by_xpath('//*[@id=\"datasetForm:tabView:filesTable:1:j_idt1227\"]')\n",
    "driver.execute_script(\"arguments[0].click()\", download)\n",
    "driver.quit()\n",
    "\n",
    "os.rename('/Users/samismalling/Downloads/countypres_2000-2016.csv', '/Users/samismalling/Documents/mobility-report-data-extractor-master/CovidMobile/data/countypres_2000-2016.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Food Environment Atlas, USDA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.ers.usda.gov/webdocs/DataFiles/48731/DataDownload.xls?v=0\n"
     ]
    }
   ],
   "source": [
    "#Downloads into .xls file because the data comes in multiple sheets \n",
    "usda_files = os.getcwd()+'/data'\n",
    "my_url = 'https://www.ers.usda.gov/data-products/food-environment-atlas/data-access-and-documentation-downloads/'\n",
    "\n",
    "html= r.get(my_url)\n",
    "soup = bs(html.content,'lxml')\n",
    "current_link = ''\n",
    "for link in soup.find_all('a'):\n",
    "    current_link = link.get('href')\n",
    "    if 'DataDownload' in current_link:\n",
    "        dwnld_link = 'https://www.ers.usda.gov'+current_link\n",
    "        print(dwnld_link)\n",
    "        myfile = r.get(dwnld_link)\n",
    "        open('{}/{}'.format(usda_files, 'food_env_atlas_usda.xls'),'wb').write(myfile.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apple Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://covid19-static.cdn-apple.com/covid19-mobility-data/2005HotfixDev13/v1/en-us/applemobilitytrends-2020-04-21.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'covid19-static.cdn-apple.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "C:\\Users\\Noah\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'covid19-static.cdn-apple.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404 1\n",
      "https://covid19-static.cdn-apple.com/covid19-mobility-data/2005HotfixDev13/v1/en-us/applemobilitytrends-2020-04-20.csv\n",
      "404 2\n",
      "https://covid19-static.cdn-apple.com/covid19-mobility-data/2005HotfixDev13/v1/en-us/applemobilitytrends-2020-04-19.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'covid19-static.cdn-apple.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "C:\\Users\\Noah\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'covid19-static.cdn-apple.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404 3\n",
      "https://covid19-static.cdn-apple.com/covid19-mobility-data/2005HotfixDev13/v1/en-us/applemobilitytrends-2020-04-18.csv\n",
      "404 4\n",
      "https://covid19-static.cdn-apple.com/covid19-mobility-data/2005HotfixDev13/v1/en-us/applemobilitytrends-2020-04-17.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'covid19-static.cdn-apple.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404 5\n",
      "https://covid19-static.cdn-apple.com/covid19-mobility-data/2005HotfixDev13/v1/en-us/applemobilitytrends-2020-04-16.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'covid19-static.cdn-apple.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404 6\n",
      "https://covid19-static.cdn-apple.com/covid19-mobility-data/2005HotfixDev13/v1/en-us/applemobilitytrends-2020-04-15.csv\n",
      "404 7\n",
      "https://covid19-static.cdn-apple.com/covid19-mobility-data/2005HotfixDev13/v1/en-us/applemobilitytrends-2020-04-14.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'covid19-static.cdn-apple.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "C:\\Users\\Noah\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'covid19-static.cdn-apple.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404 8\n",
      "https://covid19-static.cdn-apple.com/covid19-mobility-data/2005HotfixDev13/v1/en-us/applemobilitytrends-2020-04-13.csv\n",
      "200 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'covid19-static.cdn-apple.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    }
   ],
   "source": [
    "status = 0\n",
    "attempt = 0\n",
    "while status != 200:\n",
    "    appl_day = str(datetime.datetime.now().date() - datetime.timedelta(attempt))\n",
    "    url = \"https://covid19-static.cdn-apple.com/covid19-mobility-data/2005HotfixDev13/v1/en-us/applemobilitytrends-{}.csv\".format(appl_day)\n",
    "    print(url)\n",
    "    status = r.get(url, verify=False, stream=True).status_code\n",
    "    attempt = attempt + 1\n",
    "    print(status,attempt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple_data_2020-04-13.csv does not exist\n",
      "apple_data_2020-04-13.csv\n",
      "https://covid19-static.cdn-apple.com/covid19-mobility-data/2005HotfixDev13/v1/en-us/applemobilitytrends-2020-04-13.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'covid19-static.cdn-apple.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    }
   ],
   "source": [
    "apple_file = 'apple_data_{}.csv'.format(appl_day)\n",
    "create_stuff(apple_file, url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Census Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "start = 'https://web.archive.org/web/20150807220054/http://quickfacts.census.gov/qfd/download_data.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'web.archive.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"download/DataSet.txt\">DataSet.txt</a>\n",
      "DataSet.txt_2020-04-21 does not exist\n",
      "DataSet.txt_2020-04-21\n",
      "https://web.archive.org/web/20150821182814if_/http://quickfacts.census.gov:80/qfd/download/DataSet.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'web.archive.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"download/DataDict.txt\">DataDict.txt</a>\n",
      "DataDict.txt_2020-04-21 does not exist\n",
      "DataDict.txt_2020-04-21\n",
      "https://web.archive.org/web/20150821061818if_/http://quickfacts.census.gov:80/qfd/download/DataDict.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'web.archive.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"download/FIPS_CountyName.txt\">FIPS_CountyName.txt</a>\n",
      "FIPS_CountyName.txt_2020-04-21 does not exist\n",
      "FIPS_CountyName.txt_2020-04-21\n",
      "https://web.archive.org/web/20150829021807if_/http://quickfacts.census.gov:80/qfd/download/FIPS_CountyName.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'web.archive.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"download/DataFlags.txt\">DataFlags.txt</a>\n",
      "DataFlags.txt_2020-04-21 does not exist\n",
      "DataFlags.txt_2020-04-21\n",
      "https://web.archive.org/web/20150905123843if_/http://quickfacts.census.gov/qfd/download/DataFlags.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'web.archive.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"download/FlagsDict.txt\">FlagsDict.txt</a>\n",
      "FlagsDict.txt_2020-04-21 does not exist\n",
      "FlagsDict.txt_2020-04-21\n",
      "https://web.archive.org/web/20150905123459if_/http://quickfacts.census.gov/qfd/download/FlagsDict.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'web.archive.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"download/FootNotes.txt\">FootNotes.txt</a>\n",
      "FootNotes.txt_2020-04-21 does not exist\n",
      "FootNotes.txt_2020-04-21\n",
      "https://web.archive.org/web/20150905124937if_/http://quickfacts.census.gov/qfd/download/FootNotes.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'web.archive.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"download/FootText.txt\">FootText.txt</a>\n",
      "FootText.txt_2020-04-21 does not exist\n",
      "FootText.txt_2020-04-21\n",
      "https://web.archive.org/web/20150717163825if_/http://quickfacts.census.gov:80/qfd/download/FootText.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'web.archive.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    }
   ],
   "source": [
    "res = r.get(start, verify=False)\n",
    "soup = bs(res.content,'html.parser')\n",
    "sys.path.insert(0, \"/notebooks\")\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(start)\n",
    "\n",
    "\n",
    "\n",
    "for link in soup.findAll('a', href=True):\n",
    "    if '.txt' in link['href']:\n",
    "        url = link['href']\n",
    "        print(link)\n",
    "        driver.find_element_by_xpath('//a[@href=\"'+link['href']+'\"]').click()\n",
    "        url = driver.find_element_by_id(\"playback\").get_attribute(\"src\")\n",
    "        create_stuff('{}_{}'.format(link['href'].rsplit('/')[-1],str(datetime.datetime.now().date())),url)\n",
    "        driver.execute_script(\"window.history.go(-1)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USDA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Education\n",
      "Unemployment\n",
      "PovertyEstimates\n",
      "PopulationEstimates\n"
     ]
    }
   ],
   "source": [
    "usda = 'https://www.ers.usda.gov/'\n",
    "location = 'data-products/county-level-data-sets/download-data/'\n",
    "res = r.get(usda+location)\n",
    "soup = bs(res.content,'html.parser')\n",
    "for hr in soup.findAll('a',href=True):\n",
    "    if \".xls\" in hr['href']:\n",
    "        print(hr['href'].rsplit('/')[-1].split('.')[0])\n",
    "        df = pd.read_excel(usda+hr['href'])\n",
    "        head = np.where(df.T.nunique()==max(df.T.nunique()))[0][0]+1\n",
    "        df = pd.read_excel(usda+hr['href'],header=head)\n",
    "        df.to_csv('data/{}.csv'.format(hr['href'].rsplit('/')[-1].split('.')[0]),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZIP Codes to County File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using crosswalk from this source: https://www.huduser.gov/portal/datasets/usps_crosswalk.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "\n",
    "url= 'https://www.kaggle.com/danofer/zipcodes-county-fips-crosswalk'\n",
    "driver = webdriver.Safari()\n",
    "driver.get(url)\n",
    "driver.find_element_by_link_text(\"Download (2 MB)\").click()\n",
    "#log into Kaggle\n",
    "driver.find_element_by_link_text(\"Sign in with your email\")\n",
    "username = driver.find_element_by_id(\"textfield-qolce1ya1w\")\n",
    "username.clear()\n",
    "username.send_keys(\"ss2676@cornell.edu\")\n",
    "\n",
    "password = driver.find_element_by_name(\"pwd\")\n",
    "password.clear()\n",
    "password.send_keys(\"catskillz\")\n",
    "#download = driver.find_element_by_xpath('//a[@href=\"/danofer/zipcodes-county-fips-crosswalk/download\"]')\n",
    "#driver.execute_script(\"arguments[0].click()\", download)\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "\n",
    "url= 'https://www.huduser.gov/portal/datasets/usps_crosswalk.html#data'\n",
    "driver = webdriver.Safari()\n",
    "driver.get(url)\n",
    "\n",
    "select1 = Select(driver.find_element_by_id(\"crosswalk\"))\n",
    "select1.select_by_visible_text(\"ZIP-COUNTY\")\n",
    "\n",
    "select2 = Select(driver.find_element_by_id(\"year\"))\n",
    "select2.select_by_visible_text(\"1st Quarter 2020\")\n",
    "\n",
    "#time.sleep(5)\n",
    "\n",
    "download = driver.find_element_by_xpath('//input[@onclick=\"downloadFile()\"]')\n",
    "driver.execute_script(\"arguments[0].click()\", download)\n",
    "driver.quit()\n",
    "\n",
    "#os.rename('/Users/samismalling/Downloads/countypres_2000-2016.csv', '/Users/samismalling/Documents/mobility-report-data-extractor-master/CovidMobile/data/countypres_2000-2016.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<selenium.webdriver.remote.webelement.WebElement (session=\"4B5EFD26-7657-4022-A3ED-07D02FEA4132\", element=\"node-58E4F2FF-32CE-4C1D-8476-35D2A675A590\")>\n"
     ]
    }
   ],
   "source": [
    "print(download)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New York Times - Covid Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nyt = 'https://github.com/nytimes/covid-19-data'\n",
    "res = r.get(nyt)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://raw.githubusercontent.com/nytimes/covid-19-data/master/us.csv\n",
      "/2020-04-21_us.csv does not exist\n",
      "/2020-04-21_us.csv\n",
      "https://raw.githubusercontent.com/nytimes/covid-19-data/master/us.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'raw.githubusercontent.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "C:\\Users\\Noah\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'raw.githubusercontent.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-states.csv\n",
      "/2020-04-21_us-states.csv does not exist\n",
      "/2020-04-21_us-states.csv\n",
      "https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-states.csv\n",
      "https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv\n",
      "/2020-04-21_us-counties.csv does not exist\n",
      "/2020-04-21_us-counties.csv\n",
      "https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'raw.githubusercontent.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://raw.githubusercontent.com/nytimes/covid-19-data/master/us.csv\n",
      "/2020-04-21_us.csv\n",
      "https://raw.githubusercontent.com/nytimes/covid-19-data/master/us.csv\n",
      "https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-states.csv\n",
      "/2020-04-21_us-states.csv\n",
      "https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-states.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'raw.githubusercontent.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "C:\\Users\\Noah\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'raw.githubusercontent.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv\n",
      "/2020-04-21_us-counties.csv\n",
      "https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'raw.githubusercontent.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    }
   ],
   "source": [
    "soup = bs(res.content,'html.parser')\n",
    "for hr in soup.findAll('a',href=True):\n",
    "    if '/nytimes/covid-19-data/master/' in hr['href']:\n",
    "        print(hr['href'])\n",
    "        create_stuff('/{}_{}'.format(str(datetime.datetime.now().date()),hr['href'].rsplit('/')[-1]),hr['href'])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neighboring county map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = 'https://www2.census.gov/geo/docs/reference/county_adjacency.txt'\n",
    "df = pd.read_csv(url,delimiter='\\t',encoding='ISO-8859-1',header=None)\n",
    "df = df.ffill()\n",
    "df.columns = ['county','FIPS','nearby_county','nearby_FIPS'] \n",
    "df.to_csv('data/county_adjacency.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wikipedia dates for emergency declarations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "Wiki_t = \"https://en.wikipedia.org/wiki/U.S._state_and_local_government_response_to_the_2020_coronavirus_pandemic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def grab_col_headers(tup_list, x, y,heads):\n",
    "    for element in tup_list:\n",
    "        if element[0] == x:\n",
    "            for el in y:\n",
    "                heads.extend([el[1]])\n",
    "            continue\n",
    "        else:\n",
    "            heads.extend([element[1]])\n",
    "    return heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "res = r.get(Wiki_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<th colspan=\"2\" rowspan=\"2\">State/territory\n",
      "</th>\n",
      "<th rowspan=\"2\">State of emergency declared\n",
      "</th>\n",
      "<th rowspan=\"2\"><a href=\"/wiki/Stay-at-home_order\" title=\"Stay-at-home order\">Stay at home ordered</a>\n",
      "</th>\n",
      "<th rowspan=\"2\">Gatherings banned\n",
      "</th>\n",
      "<th rowspan=\"2\">Out-of-state travel restrictions\n",
      "</th>\n",
      "<th colspan=\"4\">Closures ordered\n",
      "</th>\n",
      "<th rowspan=\"2\">Sources\n",
      "</th>\n",
      "<th>Schools</th>\n",
      "<th>Daycares</th>\n",
      "<th>Bars &amp; sit-down restaurants</th>\n",
      "<th>Non-essential retail\n",
      "</th>\n",
      "['State/territory\\n', 'State of emergency declared\\n', 'Stay at home ordered\\n', 'Gatherings banned\\n', 'Out-of-state travel restrictions\\n', 'Schools', 'Daycares', 'Bars & sit-down restaurants', 'Non-essential retail\\n', 'Sources\\n']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "soup = bs(res.content,'lxml')\n",
    "cols = []\n",
    "index = []\n",
    "tmp = soup.find('table', class_='wikitable').findAll('tr')\n",
    "\n",
    "first = tmp[0]\n",
    "    \n",
    "header = []\n",
    "heads=[]\n",
    "if first.find('th').has_attr(\"rowspan\"):\n",
    "    rowspn = int(first.find('th')['rowspan'])\n",
    "    first = tmp[0:rowspn]\n",
    "    allRows = tmp[rowspn:-1]\n",
    "    for th_no, data in enumerate(first):\n",
    "        for td_no, row in enumerate(data.findAll('th')):\n",
    "            print(row)\n",
    "            if th_no == 0:\n",
    "                if row.has_attr(\"colspan\"):\n",
    "                    header.append(((int(row['colspan'])), row.text))\n",
    "                else:\n",
    "                    header.append((0, row.text))\n",
    "            else:\n",
    "                cols.append((0,row.text))\n",
    "    header = grab_col_headers(header,max([n[0] for n in header]),cols,heads)            \n",
    "            \n",
    "    print(header)       \n",
    "else:\n",
    "    first = tmp[0]\n",
    "    header = [header.get_text() for header in first.find_all('th')]\n",
    "    allRows = tmp[1:-1]\n",
    "\n",
    "results = []\n",
    "for row in allRows:\n",
    "    vals =[]\n",
    "    for v in row:       \n",
    "        if isinstance(v, NavigableString):\n",
    "            continue\n",
    "        if isinstance(v, Tag):\n",
    "            vals.extend([v.text])\n",
    "    results.append(vals)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data=results, columns=header)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df.replace(regex={r'\\n': ''},inplace=True)\n",
    "df.columns = pd.Series(df.columns.values).str.replace('\\n','')\n",
    "df.to_csv('data/emergency_orders.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updated Census Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_county_names = pd.read_csv('data/FIPS_CountyName.txt_2020-04-16', sep='delimiter', header=None)\n",
    "df_county_names = df_county_names[0].str.split(' ',n=1,expand=True)\n",
    "df_county_names = pd.concat([df_county_names[0],df_county_names[1].str.split(',',n=1,expand=True)],axis=1)\n",
    "df_county_names.columns = ['FIPS','County','State']\n",
    "df_county_names['State'] = df_county_names['State'].str.replace(' ', '')\n",
    "df_county_names.shape,df_county_names.head()\n",
    "df_county_names['state'] = df_county_names['State'].apply(replace_acronym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_county_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_county_names['strng'] = df_county_names['County'].str.replace(' ','').str.lower() + df_county_names['state'].str.replace(' ','').str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "key_file = open(\"census_api.txt\")\n",
    "lines = key_file.readlines()\n",
    "census_api_key = lines[0].rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#allows us to store results of API call cleanly\n",
    "import json\n",
    "\n",
    "'''#get list of all zipcodes in Los Angeles County separated by commas\n",
    "laZips = open('laZips.txt', 'r').readlines()\n",
    "laZips = [z.replace('\\n', '') for z in laZips]\n",
    "laZips = ','.join(laZips)'''\n",
    "\n",
    "#put your census API key here\n",
    "apiKey = census_api_key\n",
    "\n",
    "#construct the API call we will use\n",
    "url = 'https://api.census.gov/data/2017/acs/acs1?get=NAME,B02015_009E,B02015_009M&for=county:*&key={}'.format(census_api_key)\n",
    "#call the API and collect the response\n",
    "response = r.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3143"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#allows us to store results of API call cleanly\n",
    "import json\n",
    "\n",
    "'''#get list of all zipcodes in Los Angeles County separated by commas\n",
    "laZips = open('laZips.txt', 'r').readlines()\n",
    "laZips = [z.replace('\\n', '') for z in laZips]\n",
    "laZips = ','.join(laZips)'''\n",
    "\n",
    "#put your census API key here\n",
    "apiKey = census_api_key\n",
    "\n",
    "#construct the API call we will use\n",
    "url = 'https://api.census.gov/data/2017/acs/acs1?get=NAME,B02015_009E,B02015_009M&for=county:*&key={}'.format(census_api_key)\n",
    "#call the API and collect the response\n",
    "response = requests.get(url)\n",
    "\n",
    "#load the response into a JSON, ignoring the first element which is just field labels\n",
    "formattedResponse = json.loads(response.text)[1:]\n",
    "\n",
    "#flip the order of the response from [population, zipcode] -> [zipcode, population]\n",
    "formattedResponse = [item[::-1] for item in formattedResponse]\n",
    "\n",
    "#store the response in a dataframe\n",
    "laZipPopulations = pd.DataFrame(columns=['zipcode', 'population'], data=formattedResponse)\n",
    "\n",
    "#save that dataframe to a CSV spreadsheet\n",
    "laZipPopulations.to_csv('laZipPopulations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,len(url)+1):\n",
    "    try:\n",
    "        census = 'https://www.census.gov/quickfacts/fact/table/' + ','.join(url.head(300).values) + ',US/PST045219'\n",
    "        r.get(census)\n",
    "        print(i)\n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "census = 'https://www.census.gov/quickfacts/fact/table/' + ','.join(url.tail(10).values) + ',US/PST045219'\n",
    "tst = r.get(census)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.census.gov/quickfacts/fact/table/niobraracountywyoming,parkcountywyoming,plattecountywyoming,sheridancountywyoming,sublettecountywyoming,sweetwatercountywyoming,tetoncountywyoming,uintacountywyoming,washakiecountywyoming,westoncountywyoming,US/PST045219'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "https://www.census.gov/quickfacts/fact/table/washingtoncountycolorado,washingtoncountyalabama,US/PST045219"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
