{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "from state_cleaner import *\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import json\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/samismalling/CovidMobile'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('../..')\n",
    "os.chdir('CovidMobile')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data set of WBAN IDs to county FIPS and names for extraction of data by county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = pd.read_csv('./data/mshr_enhanced.csv', low_memory = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "stations = stations[stations['STATUS'].isnull()][stations['FIPS'].notnull()][stations['GHCND_ID'].notnull()][stations['COOP_ID'].notnull()].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>SOURCE_ID</th>\n",
       "      <th>SOURCE</th>\n",
       "      <th>BEGIN_DATE</th>\n",
       "      <th>END_DATE</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>NCDCSTN_ID</th>\n",
       "      <th>ICAO_ID</th>\n",
       "      <th>WBAN_ID</th>\n",
       "      <th>FAA_ID</th>\n",
       "      <th>NWSLI_ID</th>\n",
       "      <th>WMO_ID</th>\n",
       "      <th>COOP_ID</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>GHCND_ID</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "      <th>FIPS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46</td>\n",
       "      <td>AK-PW-1</td>\n",
       "      <td>COCORAHS</td>\n",
       "      <td>20090126</td>\n",
       "      <td>99991231</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30049763</td>\n",
       "      <td>AK-PW-1</td>\n",
       "      <td>US1AKPW0001</td>\n",
       "      <td>EDNA BAY 1.3 E</td>\n",
       "      <td>EDNA BAY 1.3 E</td>\n",
       "      <td>EDNA BAY</td>\n",
       "      <td>AK</td>\n",
       "      <td>PRINCE OF WALES-OUTER KETCHIKAN</td>\n",
       "      <td>50.0</td>\n",
       "      <td>55.955264</td>\n",
       "      <td>-133.61775</td>\n",
       "      <td>2201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48</td>\n",
       "      <td>AK-SH-2</td>\n",
       "      <td>COCORAHS</td>\n",
       "      <td>20090705</td>\n",
       "      <td>99991231</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30041217</td>\n",
       "      <td>AK-SH-2</td>\n",
       "      <td>US1AKSH0002</td>\n",
       "      <td>SKAGWAY 0.5 W</td>\n",
       "      <td>SKAGWAY 0.5 W</td>\n",
       "      <td>SKAGWAY</td>\n",
       "      <td>AK</td>\n",
       "      <td>SKAGWAY-HOONAH-ANGOON</td>\n",
       "      <td>50.0</td>\n",
       "      <td>59.5201</td>\n",
       "      <td>-135.3503</td>\n",
       "      <td>2232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>AK-WH-12</td>\n",
       "      <td>COCORAHS</td>\n",
       "      <td>20160813</td>\n",
       "      <td>99991231</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30108337</td>\n",
       "      <td>AK-WH-12</td>\n",
       "      <td>US1AKWH0012</td>\n",
       "      <td>HOOPER BAY 1.2 S</td>\n",
       "      <td>HOOPER BAY 1.2 S</td>\n",
       "      <td>HOOPER BAY</td>\n",
       "      <td>AK</td>\n",
       "      <td>WADE HAMPTON</td>\n",
       "      <td>50.0</td>\n",
       "      <td>61.52285</td>\n",
       "      <td>-166.09814</td>\n",
       "      <td>2270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>AK-WP-1</td>\n",
       "      <td>COCORAHS</td>\n",
       "      <td>20101017</td>\n",
       "      <td>99991231</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30066425</td>\n",
       "      <td>AK-WP-1</td>\n",
       "      <td>US1AKWP0001</td>\n",
       "      <td>PETERSBURG 4.4 NW</td>\n",
       "      <td>PETERSBURG 4.4 NW</td>\n",
       "      <td>PETERSBURG</td>\n",
       "      <td>AK</td>\n",
       "      <td>WRANGELL-PETERSBURG</td>\n",
       "      <td>50.0</td>\n",
       "      <td>56.813902</td>\n",
       "      <td>-132.95358</td>\n",
       "      <td>2280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55</td>\n",
       "      <td>AL-AT-13</td>\n",
       "      <td>COCORAHS</td>\n",
       "      <td>20090409</td>\n",
       "      <td>99991231</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30028100</td>\n",
       "      <td>AL-AT-13</td>\n",
       "      <td>US1ALAT0013</td>\n",
       "      <td>DEATSVILLE 3.4 NW</td>\n",
       "      <td>DEATSVILLE 3.4 NW</td>\n",
       "      <td>DEATSVILLE</td>\n",
       "      <td>AL</td>\n",
       "      <td>AUTAUGA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.644383</td>\n",
       "      <td>-86.430166</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index SOURCE_ID    SOURCE  BEGIN_DATE  END_DATE STATUS NCDCSTN_ID   \\\n",
       "0     46   AK-PW-1  COCORAHS    20090126  99991231    NaN    30049763   \n",
       "1     48   AK-SH-2  COCORAHS    20090705  99991231    NaN    30041217   \n",
       "2     50  AK-WH-12  COCORAHS    20160813  99991231    NaN    30108337   \n",
       "3     53   AK-WP-1  COCORAHS    20101017  99991231    NaN    30066425   \n",
       "4     55  AL-AT-13  COCORAHS    20090409  99991231    NaN    30028100   \n",
       "\n",
       "    ICAO_ID      WBAN_ID             FAA_ID        NWSLI_ID         WMO_ID  \\\n",
       "0   AK-PW-1  US1AKPW0001     EDNA BAY 1.3 E     EDNA BAY 1.3 E    EDNA BAY   \n",
       "1   AK-SH-2  US1AKSH0002      SKAGWAY 0.5 W      SKAGWAY 0.5 W     SKAGWAY   \n",
       "2  AK-WH-12  US1AKWH0012   HOOPER BAY 1.2 S   HOOPER BAY 1.2 S  HOOPER BAY   \n",
       "3   AK-WP-1  US1AKWP0001  PETERSBURG 4.4 NW  PETERSBURG 4.4 NW  PETERSBURG   \n",
       "4  AL-AT-13  US1ALAT0013  DEATSVILLE 3.4 NW  DEATSVILLE 3.4 NW  DEATSVILLE   \n",
       "\n",
       "  COOP_ID                           COUNTY  GHCND_ID        LAT         LON  \\\n",
       "0      AK  PRINCE OF WALES-OUTER KETCHIKAN      50.0  55.955264  -133.61775   \n",
       "1      AK            SKAGWAY-HOONAH-ANGOON      50.0    59.5201   -135.3503   \n",
       "2      AK                     WADE HAMPTON      50.0   61.52285  -166.09814   \n",
       "3      AK              WRANGELL-PETERSBURG      50.0  56.813902  -132.95358   \n",
       "4      AL                          AUTAUGA       1.0  32.644383  -86.430166   \n",
       "\n",
       "   FIPS  \n",
       "0  2201  \n",
       "1  2232  \n",
       "2  2270  \n",
       "3  2280  \n",
       "4  1001  "
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations['FIPS2'] = stations['FIPS'].str.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes a while to run, use the cleaned version of the downloaded CSV\n",
    "for index,row in stations.iterrows():\n",
    "    if isinstance(row['FIPS2'], list):\n",
    "        if len(row['FIPS2'])>1:\n",
    "            newfip = [i for i in row['FIPS2'] if 4<=len(i)<=5 and i.isdigit()]\n",
    "            if len(newfip)>0:\n",
    "                stations.loc[index,'FIPS2'] = int(newfip[0])\n",
    "        else:\n",
    "            if row['FIPS2'][0].isdigit():\n",
    "                stations.loc[index,'FIPS2'] = int(row['FIPS2'][0])\n",
    "            else:\n",
    "                stations.loc[index,'FIPS2'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIPS</th>\n",
       "      <th>WBAN_ID</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2201</td>\n",
       "      <td>US1AKPW0001</td>\n",
       "      <td>PRINCE OF WALES-OUTER KETCHIKAN</td>\n",
       "      <td>55.955264</td>\n",
       "      <td>-133.61775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2232</td>\n",
       "      <td>US1AKSH0002</td>\n",
       "      <td>SKAGWAY-HOONAH-ANGOON</td>\n",
       "      <td>59.5201</td>\n",
       "      <td>-135.3503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2270</td>\n",
       "      <td>US1AKWH0012</td>\n",
       "      <td>WADE HAMPTON</td>\n",
       "      <td>61.52285</td>\n",
       "      <td>-166.09814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2280</td>\n",
       "      <td>US1AKWP0001</td>\n",
       "      <td>WRANGELL-PETERSBURG</td>\n",
       "      <td>56.813902</td>\n",
       "      <td>-132.95358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>US1ALAT0013</td>\n",
       "      <td>AUTAUGA</td>\n",
       "      <td>32.644383</td>\n",
       "      <td>-86.430166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FIPS      WBAN_ID                           COUNTY        LAT         LON\n",
       "0  2201  US1AKPW0001  PRINCE OF WALES-OUTER KETCHIKAN  55.955264  -133.61775\n",
       "1  2232  US1AKSH0002            SKAGWAY-HOONAH-ANGOON    59.5201   -135.3503\n",
       "2  2270  US1AKWH0012                     WADE HAMPTON   61.52285  -166.09814\n",
       "3  2280  US1AKWP0001              WRANGELL-PETERSBURG  56.813902  -132.95358\n",
       "4  1001  US1ALAT0013                          AUTAUGA  32.644383  -86.430166"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stations['FIPS'] = stations['FIPS2']\n",
    "stations = stations[['FIPS', 'WBAN_ID','COUNTY','LAT','LON']]\n",
    "stations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations.to_csv('./data/cleaned_noaa_stations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start here if you already have the cleaned CSV downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (1) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "stations = pd.read_csv('./data/cleaned_noaa_stations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5822\n",
      "5813\n"
     ]
    }
   ],
   "source": [
    "fips = stations['FIPS'].unique()\n",
    "print(len(fips))\n",
    "fips = [int(i) for i in fips if len(str(i))<8 and i not in [np.nan]]\n",
    "print(len(fips))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_by_county = {}\n",
    "for fip in fips:\n",
    "    stations_by_county[fip] = list(stations['WBAN_ID'][stations['FIPS']==fip].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3213"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stations_by_county)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### not really sure why this isn't the same size of the fips section, something to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_data = pd.read_csv('./data/ZIP-COUNTY-FIPS_2017-06.csv')\n",
    "#full_data = pd.read_csv('./data/compiled_2020-05-22.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ZIP</th>\n",
       "      <th>COUNTYNAME</th>\n",
       "      <th>STATE</th>\n",
       "      <th>STCOUNTYFP</th>\n",
       "      <th>CLASSFP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36003</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>AL</td>\n",
       "      <td>1001</td>\n",
       "      <td>H1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36006</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>AL</td>\n",
       "      <td>1001</td>\n",
       "      <td>H1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36067</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>AL</td>\n",
       "      <td>1001</td>\n",
       "      <td>H1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36066</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>AL</td>\n",
       "      <td>1001</td>\n",
       "      <td>H1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36703</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>AL</td>\n",
       "      <td>1001</td>\n",
       "      <td>H1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ZIP      COUNTYNAME STATE  STCOUNTYFP CLASSFP\n",
       "0  36003  Autauga County    AL        1001      H1\n",
       "1  36006  Autauga County    AL        1001      H1\n",
       "2  36067  Autauga County    AL        1001      H1\n",
       "3  36066  Autauga County    AL        1001      H1\n",
       "4  36703  Autauga County    AL        1001      H1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "county_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3223"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fips = county_data['STCOUNTYFP'].unique()\n",
    "len(fips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fips_to_zips = dict()\n",
    "for fip in fips:\n",
    "    fips_to_zips[int(fip)] = list(county_data[county_data['STCOUNTYFP']==fip]['ZIP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(stationid, datasetid, begin_date, end_date, mytoken, base_url):\n",
    "    token = {'token': mytoken}\n",
    "    \n",
    "    #passing as string instead of dict because NOAA API does not like percent encoding\n",
    "    params = 'datasetid='+str(datasetid)+'&'+'locationid='+str(locationid)+'&'+'startdate='+str(begin_date)+'&'+'enddate='+str(end_date)+'&'+'limit=15'+'&datatypeid=TMIN,TMAX,PRCP'\n",
    "    \n",
    "    r = requests.get(base_url+params, headers=token)\n",
    "    #print(\"Request status code: \"+str(r.status_code))\n",
    "\n",
    "    try:\n",
    "        #results comes in json form. Convert to dataframe\n",
    "        df = pd.DataFrame.from_dict(r.json()['results'])\n",
    "        #print(\"Successfully retrieved \"+str(len(df['station'].unique()))+\" stations\")\n",
    "        dates = pd.to_datetime(df['date'])\n",
    "\n",
    "        return df\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fip_already_exists(fip):\n",
    "    #Checks to see if the fip data is already in the database\n",
    "    if conn.noaaweather.records.count_documents({ 'FIPS': fip }, limit = 1) != 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_fip_already_exists(fip,date):\n",
    "    #Checks to see if the fip data for a specific date is already in the database\n",
    "    if conn.noaaweather.records.count_documents({ 'FIPS': fip , 'date':date+'T00:00:00'}, limit = 1) != 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admin', 'config', 'local']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pymongo\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "conn=pymongo.MongoClient()\n",
    "db = conn.noaaweather\n",
    "records = db.records\n",
    "conn.list_database_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_fips = [9001,9003,9005,9007,9009,9011,9013,9015,22075,22095,23001,23003,23005,23007,\n",
    "              23009,23011,23017,23019,23025,23031,25001,25003,25005,25009,25011,25013,25015,\n",
    "              25017,25021,25023,25025,25027,26025,39133,39141,39151,39153,39155,39165,39173,\n",
    "              40017,40021,40027,40031,40037,40047,40049,40089,40099,40101,40109,40113,40119,\n",
    "              40121,40125,40131,40143,40151,41003,41005,41007,41009,41011,41015,41017,41019,\n",
    "              41027,41029,41033,41035,41039,41041,41043,41047,41051,41053,41057,41059,41065,\n",
    "              41067,41071,42001,42003,42007,42011,42013,42017,42019,42025,42027,42029,42033,\n",
    "              42041,42043,42045,42049,42051,42055,42069,42071,42073,42075,42077,42079,42081,\n",
    "              42089,42091,42095,42101,42107,42111,42115,42123,42125]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_fips = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001\n",
      "1003\n",
      "1005\n",
      "1007\n",
      "1009\n",
      "1011\n",
      "1013\n",
      "1015\n",
      "1017\n",
      "1019\n",
      "1021\n",
      "1023\n",
      "1025\n",
      "1027\n",
      "1029\n",
      "1031\n",
      "1033\n",
      "1035\n",
      "1037\n",
      "1039\n",
      "1041\n",
      "1043\n",
      "1045\n",
      "1047\n",
      "1049\n",
      "1051\n",
      "1053\n",
      "1055\n",
      "1057\n",
      "1059\n",
      "1061\n",
      "1063\n",
      "1065\n",
      "1067\n",
      "1069\n"
     ]
    }
   ],
   "source": [
    "baseurl = 'https://www.ncdc.noaa.gov/cdo-web/api/v2/data?'\n",
    "mytoken = 'NVPWXeGxislWKqThiizoffNjnPtUtAcT'\n",
    "databaseid = 'GHCND'\n",
    "begindate = '2020-02-01'\n",
    "enddate = '2020-05-01'\n",
    "\n",
    "tic = time.perf_counter()\n",
    "weather = pd.DataFrame()\n",
    "for fip, codes in stations_by_county.items():\n",
    "    fip_weather = pd.DataFrame()\n",
    "    if not fip_already_exists(fip) and fip not in empty_fips: #check to make sure not double counting data\n",
    "        print(fip)\n",
    "        for code in codes:\n",
    "            try:\n",
    "                newdf = get_weather('ZIP:'+str(code), databaseid,begindate,enddate,mytoken,baseurl)\n",
    "                fip_weather = fip_weather.append(newdf)\n",
    "            except:\n",
    "                continue\n",
    "    if not fip_weather.empty:\n",
    "        #average across the county\n",
    "        precip = fip_weather[fip_weather['datatype']=='PRCP'][['date','value']].rename(columns={'value':'precipitation'})\n",
    "        min_temp = fip_weather[fip_weather['datatype']=='TMIN'][['date','value']].rename(columns={'value':'min_temp'})\n",
    "        max_temp = fip_weather[fip_weather['datatype']=='TMAX'][['date','value']].rename(columns={'value':'max_temp'})\n",
    "        w = pd.merge(precip, temp, how ='outer', on ='date') \n",
    "        avgs = pd.merge(w.groupby('date', as_index=False)['precipitation'].mean(),\n",
    "                        w.groupby('date', as_index=False)['min_temp'].mean(),\n",
    "                        w.groupby('date', as_index=False)['max_temp'].mean(),on='date')\n",
    "        avgs['FIPS']=fip\n",
    "        avgs = avgs.to_dict()\n",
    "\n",
    "        #insert records into MongoDB\n",
    "        a = [{key:value[index] for key,value in avgs.items()}\n",
    "             for index in range(max(map(len,avgs.values())))]\n",
    "        #only insert records which are not already there:\n",
    "        for line in a:\n",
    "            if not date_fip_already_exists(line['FIPS'],line['date']):\n",
    "                records.insert_one(line)\n",
    "        print(records.count())\n",
    "    else:\n",
    "        empty_fips.append(fip)\n",
    "\n",
    "toc= time.perf_counter()\n",
    "print(f\"Run time was {toc - tic:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practice- only with zip codes from FIP 1001:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zips = fips_to_zips[1001]\n",
    "len(zips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36003\n",
      "36006\n",
      "36067\n",
      "36066\n",
      "36703\n",
      "36701\n",
      "36091\n",
      "36051\n",
      "36068\n",
      "36008\n",
      "36022\n",
      "36749\n",
      "36758\n"
     ]
    }
   ],
   "source": [
    "baseurl = 'https://www.ncdc.noaa.gov/cdo-web/api/v2/data?'\n",
    "mytoken = 'NVPWXeGxislWKqThiizoffNjnPtUtAcT'\n",
    "databaseid = 'GHCND'\n",
    "begindate = '2020-03-01'\n",
    "enddate = '2020-05-01'\n",
    "weather = pd.DataFrame()\n",
    "for code in zips:\n",
    "    print(code)\n",
    "    try:\n",
    "        newdf = get_weather('ZIP:'+str(code),databaseid,begindate,enddate,mytoken,baseurl)\n",
    "        newdf.insert(0,'ZIP',code)\n",
    "        weather = weather.append(newdf)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FIPS': 1001,\n",
       " '_id': ObjectId('5eb444b4f987a45eeb9d9046'),\n",
       " 'date': '2020-03-01T00:00:00',\n",
       " 'precipitation': 0.0,\n",
       " 'temp': 33.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request status code: 200\n"
     ]
    }
   ],
   "source": [
    "baseurl = 'https://www.ncdc.noaa.gov/cdo-web/api/v2/data?'\n",
    "mytoken = 'NVPWXeGxislWKqThiizoffNjnPtUtAcT'\n",
    "databaseid = 'GHCND'\n",
    "begindate = '2020-05-01'\n",
    "enddate = '2020-05-01'\n",
    "locationid = 'ZIP:36758'\n",
    "token = {'token': mytoken}\n",
    "params = 'datasetid='+databaseid+'&locationid='+str(locationid)+'&startdate='+str(begindate)+'&enddate='+str(enddate)+'&limit=15'+'&datatypeid=TMAX,TMIN,PRCP&limit=100'\n",
    "r = requests.get(baseurl+params, headers=token)\n",
    "print(\"Request status code: \"+str(r.status_code))\n",
    "#df = pd.DataFrame.from_dict(r.json()['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasetid=GHCND&stationid=GHCND:USW00023129&startdate=2020-05-01&enddate=2020-05-01&limit=15&datatypeid=TMAX,TMIN,PRCP&limit=100\n"
     ]
    }
   ],
   "source": [
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n",
      "415\n"
     ]
    }
   ],
   "source": [
    "for station in station_ids[0:100]:\n",
    "    params = 'datasetid='+databaseid+'&stationid='+str(locationid)+'&startdate='+str(begindate)+'&enddate='+str(enddate)+'&limit=15'+'&datatypeid=TMAX,TMIN,PRCP&limit=100'\n",
    "    r = requests.get(baseurl+params, headers=token)\n",
    "    print(len(r.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = get_weather('ZIP:36003',databaseid,begindate,enddate,mytoken,baseurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip = weather[weather['datatype']=='PRCP'][['ZIP','date','value']].rename(columns={'value':'precipitation'})\n",
    "temp = weather[weather['datatype']=='TOBS'][['date','value']].rename(columns={'value':'temp'})\n",
    "w = pd.merge(precip, temp, how ='outer', on ='date') \n",
    "avgs = pd.merge(w.groupby('date', as_index=False)['precipitation'].mean(),\n",
    "                w.groupby('date', as_index=False)['temp'].mean(), on='date')\n",
    "a = avgs.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'date': '2020-03-01T00:00:00', 'precipitation': 0, 'temp': nan},\n",
       " {'date': '2020-03-02T00:00:00', 'precipitation': 127, 'temp': nan},\n",
       " {'date': '2020-03-03T00:00:00', 'precipitation': 592, 'temp': nan},\n",
       " {'date': '2020-03-04T00:00:00', 'precipitation': 483, 'temp': nan},\n",
       " {'date': '2020-03-05T00:00:00', 'precipitation': 376, 'temp': nan},\n",
       " {'date': '2020-03-06T00:00:00', 'precipitation': 0, 'temp': nan},\n",
       " {'date': '2020-03-07T00:00:00', 'precipitation': 0, 'temp': nan},\n",
       " {'date': '2020-03-08T00:00:00', 'precipitation': 0, 'temp': nan},\n",
       " {'date': '2020-03-09T00:00:00', 'precipitation': 0, 'temp': nan},\n",
       " {'date': '2020-03-10T00:00:00', 'precipitation': 81, 'temp': nan},\n",
       " {'date': '2020-03-11T00:00:00', 'precipitation': 3, 'temp': nan},\n",
       " {'date': '2020-03-12T00:00:00', 'precipitation': 0, 'temp': nan},\n",
       " {'date': '2020-03-13T00:00:00', 'precipitation': 0, 'temp': nan},\n",
       " {'date': '2020-03-14T00:00:00', 'precipitation': 0, 'temp': nan},\n",
       " {'date': '2020-03-15T00:00:00', 'precipitation': 0, 'temp': nan},\n",
       " {'date': '2020-03-16T00:00:00', 'precipitation': 0, 'temp': nan},\n",
       " {'date': '2020-03-17T00:00:00', 'precipitation': 0, 'temp': nan},\n",
       " {'date': '2020-03-18T00:00:00', 'precipitation': 0, 'temp': nan},\n",
       " {'date': '2020-03-19T00:00:00', 'precipitation': 0, 'temp': nan},\n",
       " {'date': '2020-03-20T00:00:00', 'precipitation': 0, 'temp': nan},\n",
       " {'date': '2020-03-21T00:00:00', 'precipitation': 76, 'temp': nan},\n",
       " {'date': '2020-03-22T00:00:00', 'precipitation': 48, 'temp': nan},\n",
       " {'date': '2020-03-23T00:00:00', 'precipitation': 3, 'temp': nan},\n",
       " {'date': '2020-03-24T00:00:00', 'precipitation': 0, 'temp': nan},\n",
       " {'date': '2020-03-25T00:00:00', 'precipitation': 0, 'temp': nan}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ld = [{key:value[index] for key,value in a.items()}\n",
    "         for index in range(max(map(len,a.values())))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'date': '2020-03-01T00:00:00', 'precipitation': 0, 'temp': nan}\n",
      "{'date': '2020-03-02T00:00:00', 'precipitation': 127, 'temp': nan}\n",
      "{'date': '2020-03-03T00:00:00', 'precipitation': 592, 'temp': nan}\n",
      "{'date': '2020-03-04T00:00:00', 'precipitation': 483, 'temp': nan}\n",
      "{'date': '2020-03-05T00:00:00', 'precipitation': 376, 'temp': nan}\n",
      "{'date': '2020-03-06T00:00:00', 'precipitation': 0, 'temp': nan}\n",
      "{'date': '2020-03-07T00:00:00', 'precipitation': 0, 'temp': nan}\n",
      "{'date': '2020-03-08T00:00:00', 'precipitation': 0, 'temp': nan}\n",
      "{'date': '2020-03-09T00:00:00', 'precipitation': 0, 'temp': nan}\n",
      "{'date': '2020-03-10T00:00:00', 'precipitation': 81, 'temp': nan}\n",
      "{'date': '2020-03-11T00:00:00', 'precipitation': 3, 'temp': nan}\n",
      "{'date': '2020-03-12T00:00:00', 'precipitation': 0, 'temp': nan}\n",
      "{'date': '2020-03-13T00:00:00', 'precipitation': 0, 'temp': nan}\n",
      "{'date': '2020-03-14T00:00:00', 'precipitation': 0, 'temp': nan}\n",
      "{'date': '2020-03-15T00:00:00', 'precipitation': 0, 'temp': nan}\n",
      "{'date': '2020-03-16T00:00:00', 'precipitation': 0, 'temp': nan}\n",
      "{'date': '2020-03-17T00:00:00', 'precipitation': 0, 'temp': nan}\n",
      "{'date': '2020-03-18T00:00:00', 'precipitation': 0, 'temp': nan}\n",
      "{'date': '2020-03-19T00:00:00', 'precipitation': 0, 'temp': nan}\n",
      "{'date': '2020-03-20T00:00:00', 'precipitation': 0, 'temp': nan}\n",
      "{'date': '2020-03-21T00:00:00', 'precipitation': 76, 'temp': nan}\n",
      "{'date': '2020-03-22T00:00:00', 'precipitation': 48, 'temp': nan}\n",
      "{'date': '2020-03-23T00:00:00', 'precipitation': 3, 'temp': nan}\n",
      "{'date': '2020-03-24T00:00:00', 'precipitation': 0, 'temp': nan}\n",
      "{'date': '2020-03-25T00:00:00', 'precipitation': 0, 'temp': nan}\n"
     ]
    }
   ],
   "source": [
    "for line in ld:\n",
    "    if line['date'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export data to CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_sample = list(records.find( {} ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pd.DataFrame(columns=[])\n",
    "for num, doc in enumerate(weather_sample):\n",
    "    # convert ObjectId() to str \n",
    "    doc[\"_id\"] = str(doc[\"_id\"]) \n",
    "\n",
    "    # get document _id from dict \n",
    "    doc_id = doc[\"_id\"]\n",
    "    \n",
    "    # create a Series obj from the MongoDB dict \n",
    "    series_obj = pd.Series( doc, name=doc_id ) \n",
    "\n",
    "    # append the MongoDB Series obj to the DataFrame obj \n",
    "    docs = docs.append( series_obj )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del docs['_id']\n",
    "docs.reset_index(inplace=False)\n",
    "docs = docs.astype({'date':'str'})\n",
    "docs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.to_csv('./SafeGraph_data/NOAA_weather_sample.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check whether weather from one ZIP is different across a county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.f_oneway(weather['precipitation'][weather['ZIP'] == weather['ZIP'].unique()[0]],\n",
    "               weather['precipitation'][weather['ZIP'] == weather['ZIP'].unique()[1]],\n",
    "               weather['precipitation'][weather['ZIP'] == weather['ZIP'].unique()[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.f_oneway(weather['temp'][weather['ZIP'] == weather['ZIP'].unique()[0]].dropna(), \n",
    "             weather['temp'][weather['ZIP'] == weather['ZIP'].unique()[1]].dropna(),\n",
    "             weather['temp'][weather['ZIP'] == weather['ZIP'].unique()[2]].dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting weather data using the noaa package from github:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = noaa.NOAA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = n.get_observations(20001, 'US',start='2020-04-28',end='2020-04-30')\n",
    "f = pd.DataFrame(forecast)\n",
    "items = ['timestamp','textDescription','temperature','precipitationLast6Hours','relativeHumidity','cloudLayers']\n",
    "f = f[items]\n",
    "f.insert(0,'ZIP',zip_code) #add zip code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_weather = pd.DataFrame(columns = ['ZIP','timestamp','textDescription','temperature','precipitationLast6Hours','relativeHumidity','cloudLayers'])\n",
    "dczips = [20001,20004, 20005, 20036, 20009, 20007, 20010, 20008, 20016]\n",
    "for code in dczips:\n",
    "    forecast = n.get_observations(code, 'US',start='2020-04-28',end='2020-04-30')\n",
    "    for obs in forecast:\n",
    "        time = obs['timestamp']\n",
    "        desc = obs['textDescription']\n",
    "        temp = obs['temperature']['value']\n",
    "        precip = obs['precipitationLast6Hours']['value']\n",
    "        hum = obs['relativeHumidity']['value']\n",
    "        cloud = obs['cloudLayers'][0]['base']['value']\n",
    "    \n",
    "        row = [code,time,desc,temp,precip,hum,cloud]\n",
    "        weather.loc[len(weather)] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.to_csv('/Users/samismalling/Documents/mobility-report-data-extractor-master/CovidMobile/data/DC_weather_04_28-04_30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.DataFrame()\n",
    "bad_zips = []\n",
    "for zip_code in fips_to_county['ZIP'][0:200]:\n",
    "    if zip_code not in bad_zips:\n",
    "        print(zip_code)\n",
    "        try:\n",
    "            forecast = n.get_forecasts(zip_code, 'US',hourly = False)\n",
    "            f = pd.DataFrame(forecast)\n",
    "            f = f[['startTime','endTime','temperature','windSpeed','windDirection','shortForecast']]\n",
    "            f.insert(0,'ZIP',zip_code) #add zip code \n",
    "            weather = pd.concat([weather, f], ignore_index = True)\n",
    "        except:\n",
    "            bad_zips.append(zip_code)\n",
    "            continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
